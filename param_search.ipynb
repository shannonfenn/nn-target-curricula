{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import learn_ann\n",
    "import keras\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "import os.path\n",
    "from boolnet.utils import NumpyAwareJSONEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 'Nh': 60,\n",
    "# 'optimizer': 'Nadam',\n",
    "# 'nonlinearity': 'tanh',\n",
    "# 'loss': 'mse', "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan:\n",
    " - random search as per: http://www.jmlr.org/papers/v13/bergstra12a.html\n",
    " - choose single Ne near middle of transition band for each function\n",
    " - Nh in range(30, 120)\n",
    " - opt in [RMSprop, Adadelta, Nadam]\n",
    " - nonlin = tanh\n",
    " - loss in [mse, hinge, squared_hinge, binary_crossentropy, logcosh]\n",
    " - how to deal with epochs?   1000 = 6-8 sec,   10000 = 60-80 sec\n",
    " - no batch normalisation (we want binary extremes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently doing full-batch, but LeCun recommends Stochastic (or mini-batch)\n",
    "http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EarlyStopping\n",
    "\n",
    "`keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto')`\n",
    "\n",
    "need `validation_split`/`validation_data` in `fit()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Optimiser selection:\n",
    "\n",
    "### SGD\n",
    "probably worth checking\n",
    " - lr: float >= 0. Learning rate.\n",
    " - momentum: float >= 0. Parameter updates momentum.\n",
    " - decay: float >= 0. Learning rate decay over each update.\n",
    " - nesterov: boolean. Whether to apply Nesterov momentum.\n",
    "\n",
    "\n",
    "### RMSprop\n",
    "recommended leave parameters default (except learning rate, which can be freely tuned).\n",
    "This optimizer is usually a good choice for recurrent neural networks.\n",
    " - lr: float >= 0. Learning rate.\n",
    " - rho: float >= 0.\n",
    " - epsilon: float >= 0. Fuzz factor.\n",
    " - decay: float >= 0. Learning rate decay over each update.\n",
    "\n",
    "\n",
    "### Adadelta\n",
    "It is recommended to leave the parameters of this optimizer at their default values.\n",
    " - lr: float >= 0. Learning rate. It is recommended to leave it at the default value.\n",
    " - rho: float >= 0.\n",
    " - epsilon: float >= 0. Fuzz factor.\n",
    " - decay: float >= 0. Learning rate decay over each update.\n",
    "\n",
    "\n",
    "### Nadam\n",
    "Default parameters follow those provided in the paper.\n",
    "It is recommended to leave the parameters of this optimizer at their default values.\n",
    " - lr: float >= 0. Learning rate.\n",
    " - beta_1/beta_2: floats, 0 < beta < 1. Generally close to 1.\n",
    " - epsilon: float >= 0. Fuzz factor.\n",
    "\n",
    "\n",
    "### Adagrad\n",
    "use Adadelta or RMSprop instead\n",
    "### Adam\n",
    "Use Nadam instead\n",
    "### Adamax\n",
    "Use Nadam instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def nested_set(dic, keys, value):\n",
    "    for key in keys[:-1]:\n",
    "        dic = dic.setdefault(key, {})\n",
    "    dic[keys[-1]] = value\n",
    "\n",
    "def search(base_params, generators, n, stream):\n",
    "    for i in range(n):\n",
    "        record = {}\n",
    "        for keys, gen in generators.items():\n",
    "            val = gen()\n",
    "            nested_set(base_params, keys, val)\n",
    "            record[' '.join(keys)] = val\n",
    "        result = learn_ann.learn(base_params)\n",
    "        record['time'] = result['learning_time']\n",
    "        record['trg_err'] = result['trg_err']\n",
    "        record['trg_errs'] = result['trg_errs']\n",
    "        record['test_err'] = result['test_err']\n",
    "        record['test_errs'] = result['test_errs']\n",
    "        record['trg_mcc'] = result['trg_mcc']\n",
    "        record['trg_mccs'] = result['trg_mccs']\n",
    "        record['test_mcc'] = result['test_mcc']\n",
    "        record['test_mccs'] = result['test_mccs']\n",
    "        stream.write(',' if i else '[')\n",
    "        json.dump(record, stream, cls=NumpyAwareJSONEncoder)\n",
    "        stream.write('\\n')\n",
    "        K.clear_session()\n",
    "    stream.write(']\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# searchspace = {\n",
    "#     ('Nh', ): range(30, 121),\n",
    "#     ('optimizer', ): ['SGD', 'RMSprop', 'Adadelta', 'Nadam'],\n",
    "#     ('loss', ): ['mse', 'hinge', 'squared_hinge', 'binary_crossentropy', 'logcosh'],\n",
    "#     ('fit', 'batch_size'): [8, 16, 32, 64],\n",
    "#     ('fit', 'epochs'): [1000, 2000, 4000, 8000],\n",
    "# }\n",
    "searchspace = {\n",
    "    ('optimizer', ): ['SGD', 'RMSprop', 'Adadelta', 'Nadam'],\n",
    "#     ('sampling', 'Ne'): [8, 16, 32, 64, 128],\n",
    "    ('architecture', 'name'): ['shared_a', 'shared_b', 'shared_c', 'parallel', 'layered'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sweeps/add5_50_0\n"
     ]
    }
   ],
   "source": [
    "problem = 'add5'\n",
    "run_description = f'{problem}_{n}'\n",
    "\n",
    "base_params = {\n",
    "    'data': {\n",
    "        'filename': f'/home/shannon/HMRI/experiments/datasets/functions/{problem}.npz'\n",
    "    },\n",
    "    'sampling': {\n",
    "        'Ne': 64,\n",
    "        'seed': None\n",
    "    },\n",
    "    'architecture': {\n",
    "#         'name': 'shared_a',\n",
    "        'params': {}\n",
    "    },\n",
    "    'fit': {\n",
    "         'epochs': 5000,\n",
    "#          'batch_size': Ne\n",
    "    },\n",
    "    'batch_ratio': 0.5,\n",
    "    'nonlinearity': 'tanh',\n",
    "    'optimizer': 'RMSprop',\n",
    "    'loss': 'squared_hinge',\n",
    "    'Nh': 40,\n",
    "}\n",
    "\n",
    "\n",
    "name = next(f'sweeps/{run_description}_{i}'\n",
    "            for i in range(10000)\n",
    "            if not os.path.exists(f'sweeps/{run_description}_{i}'))\n",
    "\n",
    "os.makedirs(name)\n",
    "print(name)\n",
    "\n",
    "with open(f'{name}/base.yaml', 'w') as f:\n",
    "    yaml.dump(base_params, stream=f)\n",
    "    \n",
    "with open(f'{name}/searchspace.yaml', 'w') as f:\n",
    "    yaml.dump(searchspace, stream=f)\n",
    "\n",
    "generators = {key: lambda o=options: np.random.choice(o)\n",
    "              for key, options in searchspace.items()}\n",
    "    \n",
    "import tensorflow as tf\n",
    "with tf.device('/cpu:0'):\n",
    "    with open(f'{name}/results.json', 'w') as f:\n",
    "        search(base_params, generators, n, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
